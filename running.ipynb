{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cameron/miniconda3/envs/cam_env_2/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n",
      "/home/cameron/miniconda3/envs/cam_env_2/lib/python3.11/site-packages/torch/_utils.py:830: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from src.transformers.models.bert import BertModel\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"zhihan1996/DNA_bert_6\", trust_remote_code=True)\n",
    "model = BertModel.from_pretrained(\"zhihan1996/DNA_bert_6\", trust_remote_code=True).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio import Entrez\n",
    "from Bio import SeqIO\n",
    "Entrez.email = \"dufault77@gmail.com\"\n",
    "def download_sequence(accession, start, end):\n",
    "    handle = Entrez.efetch(db=\"nucleotide\", id=accession, rettype=\"fasta\", strand=1, seq_start=start, seq_stop=end)\n",
    "    seq_record = SeqIO.read(handle, \"fasta\")\n",
    "    handle.close()\n",
    "    return seq_record.seq.reverse_complement()\n",
    "\n",
    "#sequence_prm1 = download_sequence('NC_000016.10', 11280831, 11281340)\n",
    "\n",
    "#sequence_ = download_sequence('NC_000016.10', 11280831, 11281340)\n",
    "#print(sequence_prm1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "501"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(11281333+3) - (11280838-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "region_mapping_list = []\n",
    "region_type_mapping_list = []\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "mapping_dict = {}\n",
    "mapping_dict_type = {}\n",
    "mapping_dict[0] = \"cls\"\n",
    "mapping_dict_type[0] = \"cls\"\n",
    "sequence_prm1 = download_sequence('NC_000016.10', 11280841-3, 11281330+3)\n",
    "for i in range(1, 195):\n",
    "    mapping_dict[i] = \"exon_1\"\n",
    "for i in range(195, 287):\n",
    "    mapping_dict[i] = \"intron_1\"\n",
    "for i in range(287, 490):\n",
    "    mapping_dict[i] = \"exon_2\"\n",
    "mapping_dict[490] = \"sep\"\n",
    "for i in range(1, 195):\n",
    "    mapping_dict_type[i] = \"exon\"\n",
    "for i in range(195, 287):\n",
    "    mapping_dict_type[i] = \"intron\"\n",
    "for i in range(287, 490):\n",
    "    mapping_dict_type[i] = \"exon\"\n",
    "mapping_dict_type[490] = \"sep\"\n",
    "\n",
    "region_mapping_list.append(mapping_dict)\n",
    "region_type_mapping_list.append(mapping_dict_type)\n",
    "\n",
    "\n",
    "mapping_dict = {}\n",
    "mapping_dict_type = {}\n",
    "mapping_dict[0] = \"cls\"\n",
    "mapping_dict_type[0] = \"cls\"\n",
    "sequence_1 = download_sequence(\"NC_000001.11\",9424658-3, 9425159+3)\n",
    "for i in range(1, 293):\n",
    "    mapping_dict[i] = \"ALU_1\"\n",
    "for i in range(293, 502):\n",
    "    mapping_dict[i] = \"MIR_1\"\n",
    "mapping_dict[502] = \"sep\"\n",
    "for i in range(1, 293):\n",
    "    mapping_dict_type[i] = \"ALU\"\n",
    "for i in range(293, 502):\n",
    "    mapping_dict_type[i] = \"MIR\"\n",
    "mapping_dict_type[502] = \"sep\"\n",
    "region_mapping_list.append(mapping_dict)\n",
    "region_type_mapping_list.append(mapping_dict_type)\n",
    "\n",
    "\n",
    "mapping_dict = {}\n",
    "mapping_dict_type = {}\n",
    "mapping_dict[0] = \"cls\"\n",
    "mapping_dict_type[0] = \"cls\"\n",
    "sequence_2 = download_sequence(\"NC_000001.11\",9530501-3, 9530700+3)\n",
    "for i in range(1, 64):\n",
    "    mapping_dict[i] = \"MIR_1\"\n",
    "for i in range(64, 200):\n",
    "    mapping_dict[i] = \"ALU_1\"\n",
    "mapping_dict[200] = \"sep\"\n",
    "for i in range(1, 64):\n",
    "    mapping_dict_type[i] = \"ALU\"\n",
    "for i in range(64, 200):\n",
    "    mapping_dict_type[i] = \"MIR\"\n",
    "mapping_dict_type[200] = \"sep\"\n",
    "region_mapping_list.append(mapping_dict)\n",
    "region_type_mapping_list.append(mapping_dict_type)\n",
    "#sequence_3\n",
    "#sequence_4\n",
    "#sequence_5\n",
    "#sequence_6\n",
    "#sequence_7\n",
    "#sequence_8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers = 12\n",
    "num_heads = 12\n",
    "skip = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_dict = {\"tokens\": list()}\n",
    "vocab_reverse = dict((value, key) for key, value in tokenizer.vocab.items())\n",
    "\n",
    "\n",
    "attention_dict = dict()\n",
    "point_position_dict = dict()\n",
    "point_position_dict = dict()\n",
    "agg_attn_dict = dict()\n",
    "for layer in range(0,num_layers,skip):\n",
    "    attention_dict[layer] = dict()\n",
    "    point_position_dict[layer] = dict()\n",
    "    for head in range(0,num_heads,skip):\n",
    "        agg_attn_dict[f\"{layer}_{head}\"] = list()\n",
    "        attention_dict[layer][head] = dict()\n",
    "        attention_dict[layer][head][\"layer\"] = layer \n",
    "        attention_dict[layer][head][\"head\"] = head\n",
    "        attention_dict[layer][head][\"tokens\"] = list() \n",
    "        point_position_dict[layer][head] = dict()\n",
    "        point_position_dict[layer][head][\"layer\"] = layer \n",
    "        point_position_dict[layer][head][\"head\"] = head\n",
    "        point_position_dict[layer][head][\"tokens\"] = list()\n",
    "        point_position_dict[layer][head][\"query\"] = list()  \n",
    "        point_position_dict[layer][head][\"key\"] = list()         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = [str(sequence_prm1), str(sequence_1), str(sequence_2)]\n",
    "\n",
    "sentence_stops = list()\n",
    "sentence_starts = list()\n",
    "pos = 0\n",
    "for seq_num,sequence in enumerate(dataset):\n",
    "    sentence_starts.append(pos)\n",
    "    inputs = tokenizer(\" \".join([sequence[i-3:i+3] for i in range(3, len(sequence)-4, 1)]), return_tensors = 'pt')\n",
    "    out = model(**inputs.to(device), output_attentions=True, return_dict=True)\n",
    "    tokens = [vocab_reverse[x] for x in inputs['input_ids'].tolist()[0]]   \n",
    "    pos = pos+len(tokens)\n",
    "    sentence_stops.append(pos) \n",
    "    for i,value in enumerate(tokens):\n",
    "        single_token = {}\n",
    "        single_token['value'] = value\n",
    "        single_token['region_type'] = region_type_mapping_list[seq_num][i]\n",
    "        single_token['region_name'] = region_mapping_list[seq_num][i]\n",
    "        single_token['type'] = \"query\"\n",
    "        single_token[\"length\"] = len(tokens)\n",
    "        single_token['pos_int'] = i\n",
    "        single_token['position'] = i/(len(tokens)- 1)\n",
    "        single_token['sentence'] = \" \".join(tokens) \n",
    "   \n",
    "        tokens_dict['tokens'].append(single_token)\n",
    "        for layer in range(0,num_layers,skip): \n",
    "            for head in range(0,num_heads,skip):\n",
    "                point_position_dict[layer][head]['query'].append(out.query[layer][head][i].detach().cpu().numpy())\n",
    "                point_position_dict[layer][head]['key'].append(out.key[layer][head][i].detach().cpu().numpy())\n",
    "                attention_dict[layer][head]['tokens'].append({'attention' : out.attentions[layer][0][head][i].detach().cpu().numpy()})\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "201"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:15<00:00,  4.00s/it]\n",
      "100%|██████████| 4/4 [00:13<00:00,  3.37s/it]\n",
      "100%|██████████| 4/4 [00:14<00:00,  3.54s/it]\n",
      "100%|██████████| 4/4 [00:14<00:00,  3.60s/it]\n",
      "100%|██████████| 4/4 [00:58<00:00, 14.51s/it]\n"
     ]
    }
   ],
   "source": [
    "# Getting point positions\n",
    "\n",
    "#Some code from chatGPT!!!!!!!\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import umap\n",
    "import numpy as np\n",
    "import tqdm\n",
    "\n",
    "def get_pca_embeddings(vectors, n_components=2):\n",
    "    pca = PCA(n_components=n_components)\n",
    "    return pca.fit_transform(vectors)\n",
    "def get_tsne_embeddings(vectors, n_components=2, perplexity=30.0):\n",
    "    tsne = TSNE(n_components=n_components, perplexity=perplexity, n_iter=250)\n",
    "    return tsne.fit_transform(vectors)\n",
    "def get_umap_embeddings(vectors, n_components=2, n_neighbors=15, min_dist=0.1):\n",
    "    umap_model = umap.UMAP(n_components=n_components, n_neighbors=n_neighbors, min_dist=min_dist)\n",
    "    return umap_model.fit_transform(vectors)\n",
    "def calculate_centroid(vectors):\n",
    "    \"\"\"Calculate the centroid of a list of vectors.\"\"\"\n",
    "    return np.mean(vectors, axis=0)\n",
    "def translate_vectors(source_vectors, target_vectors):\n",
    "    \"\"\"Translate source_vectors so their centroid matches that of target_vectors.\"\"\"\n",
    "    source_centroid = calculate_centroid(source_vectors)\n",
    "    target_centroid = calculate_centroid(target_vectors)\n",
    "    translation = target_centroid - source_centroid\n",
    "    translated_vectors = source_vectors + translation\n",
    "    return translated_vectors\n",
    "\n",
    "def calculate_norm(vector):\n",
    "    return np.linalg.norm(vector)\n",
    "\n",
    "for layer in tqdm.tqdm(range(0,num_layers,skip)): \n",
    "    for head in tqdm.tqdm(range(0,num_heads,skip)):\n",
    "        translated_key = translate_vectors(point_position_dict[layer][head]['key'], point_position_dict[layer][head]['query'])\n",
    "        vectors = np.stack(point_position_dict[layer][head]['query'] + [np.array(row) for row in translated_key])\n",
    "        \n",
    "        pca_2d = get_pca_embeddings(vectors, n_components=2)\n",
    "        #pca_3d = get_pca_embeddings(vectors, n_components=3)\n",
    "\n",
    "        tsne_2d = get_tsne_embeddings(vectors, n_components=2)\n",
    "        #tsne_3d = get_tsne_embeddings(vectors, n_components=3)\n",
    "\n",
    "        umap_2d = get_umap_embeddings(vectors, n_components=2)\n",
    "        #umap_3d = get_umap_embeddings(vectors, n_components=3)\n",
    "        \n",
    "        for token in range(umap_2d.shape[0]):\n",
    "            point_position_dict[layer][head]['tokens'].append({\n",
    "                \"tsne_x\" : tsne_2d[token][0],\n",
    "                \"tsne_y\" : tsne_2d[token][1],\n",
    "                \n",
    "                #\"tsne_x_3d\" : tsne_3d[token][0],\n",
    "                #\"tsne_y_3d\" : tsne_3d[token][1],                \n",
    "                #\"tsne_z_3d\" : tsne_3d[token][2],                \n",
    "                \n",
    "                \"umap_x\" : umap_2d[token][0],\n",
    "                \"umap_y\" : umap_2d[token][1],          \n",
    "\n",
    "                #\"umap_x_3d\" : umap_3d[token][0],\n",
    "                #\"umap_y_3d\" : umap_3d[token][1],                \n",
    "                #\"umap_z_3d\" : umap_3d[token][2],                         \n",
    "                \n",
    "                \"pca_x\" : pca_2d[token][0],\n",
    "                \"pca_y\" : pca_2d[token][1],\n",
    "                \n",
    "                #\"pca_x_3d\" : pca_3d[token][0],\n",
    "                #\"pca_y_3d\" : pca_3d[token][1],                                    \n",
    "                #\"pca_z_3d\" : pca_3d[token][2],    \n",
    "                \n",
    "                \"norm\" : calculate_norm(vectors[token])                              \n",
    "            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 18.38it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 18.92it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 17.96it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 18.33it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00,  4.56it/s]\n"
     ]
    }
   ],
   "source": [
    "for layer in tqdm.tqdm(range(0,num_layers,skip)): \n",
    "    for head in tqdm.tqdm(range(0,num_heads,skip)):\n",
    "        h = [np.stack([token['attention'] for token in attention_dict[layer][head]['tokens'][sentence_starts[i]:sentence_stops[i]]]) for i in range(len(dataset))]\n",
    "        max_h = max([x.shape[0] for x in h])\n",
    "        shaped = [np.pad(x,(0,max_h - x.shape[0])) for x in h]\n",
    "        avg =  np.average(np.stack(shaped),axis=0)\n",
    "        agg_attn_dict[f\"{layer}_{head}\"] = [{\"attention\" : [round(x,5) for x in avg[i].tolist()]} for i in range(avg.shape[0])]\n",
    "        \n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 761.32it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 790.59it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 817.36it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 646.02it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 103.22it/s]\n"
     ]
    }
   ],
   "source": [
    "for layer in tqdm.tqdm(range(0,num_layers,skip)): \n",
    "    for head in tqdm.tqdm(range(0,num_heads,skip)):\n",
    "        del point_position_dict[layer][head][\"query\"]\n",
    "        del point_position_dict[layer][head][\"key\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 149.33it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 156.47it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 156.35it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 153.40it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 35.79it/s]\n"
     ]
    }
   ],
   "source": [
    "for layer in tqdm.tqdm(range(0,num_layers,skip)): \n",
    "    for head in tqdm.tqdm(range(0,num_heads,skip)):\n",
    "        for i in range(len(point_position_dict[layer][head]['tokens'])):\n",
    "            for data_feature in [\"tsne_x\", \"tsne_y\", \"umap_x\", \"umap_y\", \"norm\", \"pca_x\", \"pca_y\"]:\n",
    "                point_position_dict[layer][head]['tokens'][i][data_feature] = round(float(point_position_dict[layer][head]['tokens'][i][data_feature]),5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1195/1195 [00:00<00:00, 6442407.81it/s]\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "tokens = copy.deepcopy(tokens_dict[\"tokens\"])\n",
    "pls = list()\n",
    "for token in tqdm.tqdm(tokens):\n",
    "    token[\"type\"] = \"key\"\n",
    "    tokens_dict[\"tokens\"].append(token)\n",
    "del tokens\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00,  8.33it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00,  8.70it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00,  8.90it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00,  8.38it/s]\n",
      "100%|██████████| 4/4 [00:01<00:00,  2.13it/s]\n"
     ]
    }
   ],
   "source": [
    "for layer in tqdm.tqdm(range(0,num_layers,skip)): \n",
    "    for head in tqdm.tqdm(range(0,num_heads,skip)):\n",
    "        for att in attention_dict[layer][head]['tokens']:\n",
    "            att['attention'] = [round(x,5) for x in att['attention'].tolist()] \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing to files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/home/cameron/repos/attention-viz-bio/web/data/DNABERT/agg_attn.json\", \"w\") as fp:\n",
    "    json.dump(agg_attn_dict , fp) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "del agg_attn_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/home/cameron/repos/attention-viz-bio/web/data/DNABERT/tokens.json\", \"w\") as fp:\n",
    "    json.dump(tokens_dict , fp) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "del tokens_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00,  4.74it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00,  4.62it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00,  4.92it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00,  5.74it/s]\n",
      "100%|██████████| 4/4 [00:03<00:00,  1.24it/s]\n"
     ]
    }
   ],
   "source": [
    "for layer in tqdm.tqdm(range(0,num_layers,skip)): \n",
    "    for head in tqdm.tqdm(range(0,num_heads,skip)):\n",
    "        with open(f\"/home/cameron/repos/attention-viz-bio/web/data/DNABERT/attention/layer{layer}_head{head}.json\", \"w\") as fp:\n",
    "            json.dump(attention_dict[layer][head] , fp) \n",
    "        with open(f\"/home/cameron/repos/attention-viz-bio/web/data/DNABERT/byLayerHead/layer{layer}_head{head}.json\", \"w\") as fp:\n",
    "            json.dump(point_position_dict[layer][head] , fp)             \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing aggregation things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_dict_aggregate = {\"tokens\": list()}\n",
    "vocab_reverse = dict((value, key) for key, value in tokenizer.vocab.items())\n",
    "\n",
    "\n",
    "attention_dict_aggregate = dict()\n",
    "point_position_dict_aggregate = dict()\n",
    "agg_attn_dict_aggregate = dict()\n",
    "for layer in range(0,num_layers,skip):\n",
    "    attention_dict_aggregate[layer] = dict()\n",
    "    point_position_dict_aggregate[layer] = dict()\n",
    "    for head in range(0,num_heads,skip):\n",
    "        agg_attn_dict_aggregate[f\"{layer}_{head}\"] = list()\n",
    "        attention_dict_aggregate[layer][head] = dict()\n",
    "        attention_dict_aggregate[layer][head][\"layer\"] = layer \n",
    "        attention_dict_aggregate[layer][head][\"head\"] = head\n",
    "        attention_dict_aggregate[layer][head][\"tokens\"] = list() \n",
    "        point_position_dict_aggregate[layer][head] = dict()\n",
    "        point_position_dict_aggregate[layer][head][\"layer\"] = layer \n",
    "        point_position_dict_aggregate[layer][head][\"head\"] = head\n",
    "        point_position_dict_aggregate[layer][head][\"tokens\"] = list()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_list = list(region_mapping_list[0].values())\n",
    "unique_list = []\n",
    "for item in original_list:\n",
    "    if item not in unique_list:\n",
    "        unique_list.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_dicts(dict_list):\n",
    "    if not dict_list:\n",
    "        return {}\n",
    "    \n",
    "    # Initialize a dictionary to store the sum of values for each key\n",
    "    sum_dict = {key: 0 for key in dict_list[0].keys()}\n",
    "\n",
    "    # Sum up the values for each key\n",
    "    for d in dict_list:\n",
    "        for key in sum_dict.keys():\n",
    "            sum_dict[key] += d[key]\n",
    "\n",
    "    # Calculate the average for each key\n",
    "    avg_dict = {key: value / len(dict_list) for key, value in sum_dict.items()}\n",
    "    #avg_dict['norm'] = sum_dict['norm']\n",
    "\n",
    "    return avg_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset = [str(sequence_prm1), str(sequence_1), str(sequence_2)]\n",
    "\n",
    "sentence_stops_aggregate = list()\n",
    "sentence_starts_aggregate = list()\n",
    "pos = 0\n",
    "for seq_num,sequence in enumerate(dataset):\n",
    "    sentence_starts_aggregate.append(pos)    \n",
    "    original_list = list(region_mapping_list[seq_num].values())\n",
    "    tokens = []\n",
    "    change_list = []\n",
    "    for i,item in enumerate(original_list):\n",
    "        if item not in tokens:\n",
    "            change_list.append(i)\n",
    "            tokens.append(item)\n",
    "    change_list.append(len(original_list))\n",
    "    pos = pos+len(tokens)\n",
    "    sentence_stops_aggregate.append(pos) \n",
    "    for i,value in enumerate(tokens):\n",
    "        single_token = {}\n",
    "        single_token['value'] = value\n",
    "        single_token['region_type'] = value.split(\"_\")[0]\n",
    "        single_token['region_name'] = value\n",
    "        single_token['type'] = \"query\"\n",
    "        single_token[\"length\"] = len(tokens)\n",
    "        single_token['pos_int'] = i\n",
    "        single_token['position'] = i/(len(tokens)- 1)\n",
    "        single_token['sentence'] = \" \".join(tokens) \n",
    "   \n",
    "        tokens_dict_aggregate['tokens'].append(single_token)\n",
    "    for layer in range(0,num_layers,skip): \n",
    "        for head in range(0,num_heads,skip):       \n",
    "            x =  [[attention_dict[layer][head]['tokens'][j+sentence_starts[seq_num]]['attention'][change_list[i]:change_list[i+1]] for i in range(len(tokens))] for j in range(len(original_list))  ]               \n",
    "            out = [[np.average(attention_dict[layer][head]['tokens'][j+sentence_starts[seq_num]]['attention'][change_list[i]:change_list[i+1]]) for i in range(len(tokens))] for j in range(len(original_list))  ]\n",
    "            for i in range(len(tokens)):\n",
    "                point_position_dict_aggregate[layer][head][\"tokens\"].append(average_dicts(point_position_dict[layer][head]['tokens'][(sentence_starts[seq_num]+change_list[i]):(sentence_starts[seq_num]+change_list[i+1])]))\n",
    "                attention_dict_aggregate[layer][head]['tokens'].append({'attention' : np.average(np.array(out[change_list[i]:change_list[i+1]]),axis=0)})\n",
    "                \n",
    "\n",
    "                \n",
    "pos = 0\n",
    "for seq_num,sequence in enumerate(dataset):\n",
    "    original_list = list(region_mapping_list[seq_num].values())\n",
    "    tokens = []\n",
    "    change_list = []\n",
    "    for i,item in enumerate(original_list):\n",
    "        if item not in tokens:\n",
    "            change_list.append(i)\n",
    "            tokens.append(item)\n",
    "    change_list.append(len(original_list))\n",
    "    for layer in range(0,num_layers,skip): \n",
    "        for head in range(0,num_heads,skip):         \n",
    "            for i in range(len(tokens)):\n",
    "                point_position_dict_aggregate[layer][head][\"tokens\"].append(average_dicts(point_position_dict[layer][head]['tokens'][(sentence_stops[-1]+sentence_starts[seq_num]+change_list[i]):(sentence_stops[-1]+sentence_starts[seq_num]+change_list[i+1])]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 3970.00it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 4300.75it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 4374.76it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 5023.12it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 361.78it/s]\n"
     ]
    }
   ],
   "source": [
    "for layer in tqdm.tqdm(range(0,num_layers,skip)): \n",
    "    for head in tqdm.tqdm(range(0,num_heads,skip)):\n",
    "        h = [np.stack([token['attention'] for token in attention_dict_aggregate[layer][head]['tokens'][sentence_starts_aggregate[i]:sentence_stops_aggregate[i]]]) for i in range(len(dataset))]\n",
    "        max_h = max([x.shape[0] for x in h])\n",
    "        shaped = [np.pad(x,(0,max_h - x.shape[0])) for x in h]\n",
    "        avg =  np.average(np.stack(shaped),axis=0)\n",
    "        agg_attn_dict_aggregate[f\"{layer}_{head}\"] = [{\"attention\" : [round(x,5) for x in avg[i].tolist()]} for i in range(avg.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 20189.19it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 20262.34it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 21620.12it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 22826.14it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 523.27it/s]\n"
     ]
    }
   ],
   "source": [
    "for layer in tqdm.tqdm(range(0,num_layers,skip)): \n",
    "    for head in tqdm.tqdm(range(0,num_heads,skip)):\n",
    "        for i in range(len(point_position_dict_aggregate[layer][head]['tokens'])):\n",
    "            for data_feature in [\"tsne_x\", \"tsne_y\", \"umap_x\", \"umap_y\", \"norm\", \"pca_x\", \"pca_y\"]:\n",
    "                point_position_dict_aggregate[layer][head]['tokens'][i][data_feature] = float(point_position_dict_aggregate[layer][head]['tokens'][i][data_feature])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:00<00:00, 235025.66it/s]\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "tokens = copy.deepcopy(tokens_dict_aggregate[\"tokens\"])\n",
    "pls = list()\n",
    "for token in tqdm.tqdm(tokens):\n",
    "    token[\"type\"] = \"key\"\n",
    "    tokens_dict_aggregate[\"tokens\"].append(token)\n",
    "del tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 42690.12it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 7064.09it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 48913.17it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 51150.05it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 523.58it/s]\n"
     ]
    }
   ],
   "source": [
    "for layer in tqdm.tqdm(range(0,num_layers,skip)): \n",
    "    for head in tqdm.tqdm(range(0,num_heads,skip)):\n",
    "        for att in attention_dict_aggregate[layer][head]['tokens']:\n",
    "            att['attention'] = att['attention'].tolist() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing Aggregate to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"/home/cameron/repos/attention-viz-bio/web/data/DNABERT_agg/agg_attn.json\", \"w\") as fp:\n",
    "    json.dump(agg_attn_dict_aggregate , fp) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/home/cameron/repos/attention-viz-bio/web/data/DNABERT_agg/tokens.json\", \"w\") as fp:\n",
    "    json.dump(tokens_dict_aggregate , fp) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 1786.14it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 1715.11it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 2002.53it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 1838.20it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 245.17it/s]\n"
     ]
    }
   ],
   "source": [
    "## Writing to files\n",
    "import json\n",
    "\n",
    "for layer in tqdm.tqdm(range(0,num_layers,skip)): \n",
    "    for head in tqdm.tqdm(range(0,num_heads,skip)):\n",
    "        with open(f\"/home/cameron/repos/attention-viz-bio/web/data/DNABERT_agg/attention/layer{layer}_head{head}.json\", \"w\") as fp:\n",
    "            json.dump(attention_dict_aggregate[layer][head] , fp) \n",
    "        with open(f\"/home/cameron/repos/attention-viz-bio/web/data/DNABERT_agg/byLayerHead/layer{layer}_head{head}.json\", \"w\") as fp:\n",
    "            json.dump(point_position_dict_aggregate[layer][head] , fp)             \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cam_env_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
