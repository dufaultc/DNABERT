{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cameron/miniconda3/envs/cam_env_2/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n",
      "/home/cameron/miniconda3/envs/cam_env_2/lib/python3.11/site-packages/torch/_utils.py:830: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from src.transformers.models.bert import BertModel\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"zhihan1996/DNA_bert_6\", trust_remote_code=True)\n",
    "model = BertModel.from_pretrained(\"zhihan1996/DNA_bert_6\", trust_remote_code=True).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers = 12\n",
    "num_heads = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_dict = {\"tokens\": list()}\n",
    "vocab_reverse = dict((value, key) for key, value in tokenizer.vocab.items())\n",
    "\n",
    "\n",
    "attention_dict = dict()\n",
    "point_position_dict = dict()\n",
    "agg_attn_dict = dict()\n",
    "for layer in range(num_layers):\n",
    "    attention_dict[layer] = dict()\n",
    "    point_position_dict[layer] = dict()\n",
    "    for head in range(num_heads):\n",
    "        agg_attn_dict[f\"{layer}_{head}\"] = list()\n",
    "        attention_dict[layer][head] = dict()\n",
    "        attention_dict[layer][head][\"layer\"] = layer \n",
    "        attention_dict[layer][head][\"head\"] = head\n",
    "        attention_dict[layer][head][\"tokens\"] = list() \n",
    "        point_position_dict[layer][head] = dict()\n",
    "        point_position_dict[layer][head][\"layer\"] = layer \n",
    "        point_position_dict[layer][head][\"head\"] = head\n",
    "        point_position_dict[layer][head][\"tokens\"] = list()\n",
    "        point_position_dict[layer][head][\"query\"] = list()  \n",
    "        point_position_dict[layer][head][\"key\"] = list()         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dna_1 = \"GGGGTAATCAGAGCAGAACCAGGCACCTGCCCTGCCTGATGTCCTCTGCTCAGGGCTGGCAGCTGTGTCCTGTGTCCTCCCCACCCCCTGGGACCACAAAGCTCCACCCCTGCCACACCCTGACATACTCAAGCCCAGGAGCCTGACCCAGGGCTCAGGGTGGGGTCAAAAACCGGGGGGATCTGATTTGCATGGATGGACTCTCCCCCTCTCAGAGTATGAAGAGAGGGAGAGATCTGGGGGAAGCTCAGCTTCAGCTGTGGTAGAGAAGACAGGATTCAGGACAATCTCCAGCATGGC\"\n",
    "dna_2 = \"AAAGAGACCCGGGGAGCATCTGGGCTTCCAAGGTCCTCGGTACGGCCCAAGGCAGCGAAGGACGCGCGGCTCCAGGCTGCGGGAGCCAGGACGACCGGGGGCTCCCAGAGCGCGAAGTCGCGATCCTCGGCGGTGGAGAGCTCGTGCCAAAACGTCCTCCCCTGCGCCAGTCAGGCCTTCGCGGGGCTGGCAGGCGGGCGGGGGCGGGGCCGCCGCACTTTAAGAGGCTGTGCAGGCAGACAGACCTCCAGGCCCGCTAGGGGATCCGCGCCATGGAGGCCGCCCGGGACTATGCAGGAG\"\n",
    "dna_3 = \"AGACCCCGGAGCCACAAGGAGAGGGCTGGATCCCCGGCTCAGAGGGAAGAGGTCGGATCCCCAGCTGAGAGGGAGGAGGGTCCCGGACCCTAGGAGTGGGAAGGAAAGGCTCGGATCCCCTGATCCCCAGGAGGAGGGGACCCGGCTGCCTCCCGGTTGGGGCCGCGCGAGGGCGGGGCGCGGAAGGATCCGGGAGGGCCGTGCTCCGCCACCCAGTATATATCTGTCCCCAGTCCCCGGGGCCGCCTCATTCCCTGTCCTCGGATCACAGTCTCTTCTCACTACAGTGTCGCCGCCTCT\"\n",
    "dna_4 = \"GTCTTTCCTTGGAGGAGGCATTGGCACGAGTTACTATAAACTCCCTCTGAATCTCAAGACTTCTGGGACGCCGATTCCGCTCCTGGCCTGGGGCAAGGCGTGGGAGCTTGGAAGCCAGCGCTGCGCTCCCCGTGGGAAGCGATCGTCTCCTCTGTCAACTCGCGCCTGGGCACTTAGCCCCTCCCGTTTCAGGGCGCCGCCTCCCCGGATGGCAAACACTATAAAGTGGCGGCGAATAAGGTTCCTCCTGCTGCTCTCGGTTTAGTCCAAGATCAGCGATATCACGCGTCCCCCGGAGCA\"\n",
    "\n",
    "dataset = [dna_1, dna_2, dna_3, dna_4]\n",
    "\n",
    "sentence_stops = list()\n",
    "sentence_starts = list()\n",
    "pos = 0\n",
    "for sequence in dataset:\n",
    "    sentence_starts.append(pos)\n",
    "    inputs = tokenizer(\" \".join([sequence[i:i+6] for i in range(0, len(sequence)-6, 1)]), return_tensors = 'pt')\n",
    "    out = model(**inputs.to(device), output_attentions=True, return_dict=True)\n",
    "    tokens = [vocab_reverse[x] for x in inputs['input_ids'].tolist()[0]]   \n",
    "    pos = pos+len(tokens)\n",
    "    sentence_stops.append(pos) \n",
    "    for i,value in enumerate(tokens):\n",
    "        single_token = {}\n",
    "        single_token['value'] = value\n",
    "        single_token['type'] = \"query\"\n",
    "        single_token[\"length\"] = len(tokens)\n",
    "        single_token['pos_int'] = i\n",
    "        single_token['position'] = i/(len(tokens)- 1)\n",
    "        single_token['sentence'] = \" \".join(tokens) \n",
    "   \n",
    "        tokens_dict['tokens'].append(single_token)\n",
    "        for layer in range(num_layers): \n",
    "            for head in range(num_heads):\n",
    "                point_position_dict[layer][head]['query'].append(out.query[layer][head][i].detach().cpu().numpy())\n",
    "                point_position_dict[layer][head]['key'].append(out.key[layer][head][i].detach().cpu().numpy())\n",
    "                attention_dict[layer][head]['tokens'].append({'attention' : out.attentions[layer][0][head][i].detach().cpu().numpy()})\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:24<00:00,  8.06s/it]\n",
      "100%|██████████| 3/3 [00:22<00:00,  7.35s/it]\n",
      "100%|██████████| 3/3 [00:23<00:00,  7.84s/it]\n",
      "100%|██████████| 3/3 [01:09<00:00, 23.25s/it]\n"
     ]
    }
   ],
   "source": [
    "# Getting point positions\n",
    "\n",
    "#Some code from chatGPT!!!!!!!\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import umap\n",
    "import numpy as np\n",
    "import tqdm\n",
    "\n",
    "def get_pca_embeddings(vectors, n_components=2):\n",
    "    pca = PCA(n_components=n_components)\n",
    "    return pca.fit_transform(vectors)\n",
    "def get_tsne_embeddings(vectors, n_components=2, perplexity=30.0):\n",
    "    tsne = TSNE(n_components=n_components, perplexity=perplexity, n_iter=250)\n",
    "    return tsne.fit_transform(vectors)\n",
    "def get_umap_embeddings(vectors, n_components=2, n_neighbors=15, min_dist=0.1):\n",
    "    umap_model = umap.UMAP(n_components=n_components, n_neighbors=n_neighbors, min_dist=min_dist)\n",
    "    return umap_model.fit_transform(vectors)\n",
    "def calculate_centroid(vectors):\n",
    "    \"\"\"Calculate the centroid of a list of vectors.\"\"\"\n",
    "    return np.mean(vectors, axis=0)\n",
    "def translate_vectors(source_vectors, target_vectors):\n",
    "    \"\"\"Translate source_vectors so their centroid matches that of target_vectors.\"\"\"\n",
    "    source_centroid = calculate_centroid(source_vectors)\n",
    "    target_centroid = calculate_centroid(target_vectors)\n",
    "    translation = target_centroid - source_centroid\n",
    "    translated_vectors = source_vectors + translation\n",
    "    return translated_vectors\n",
    "\n",
    "def calculate_norm(vector):\n",
    "    return np.linalg.norm(vector)\n",
    "\n",
    "for layer in tqdm.tqdm(range(num_layers)): \n",
    "    for head in tqdm.tqdm(range(num_heads)):\n",
    "        translated_key = translate_vectors(point_position_dict[layer][head]['key'], point_position_dict[layer][head]['query'])\n",
    "        vectors = np.stack(point_position_dict[layer][head]['query'] + [np.array(row) for row in translated_key])\n",
    "        \n",
    "        pca_2d = get_pca_embeddings(vectors, n_components=2)\n",
    "        pca_3d = get_pca_embeddings(vectors, n_components=3)\n",
    "\n",
    "        tsne_2d = get_tsne_embeddings(vectors, n_components=2)\n",
    "        tsne_3d = get_tsne_embeddings(vectors, n_components=3)\n",
    "\n",
    "        umap_2d = get_umap_embeddings(vectors, n_components=2)\n",
    "        umap_3d = get_umap_embeddings(vectors, n_components=3)\n",
    "        \n",
    "        for token in range(umap_2d.shape[0]):\n",
    "            point_position_dict[layer][head]['tokens'].append({\n",
    "                \"tsne_x\" : tsne_2d[token][0],\n",
    "                \"tsne_y\" : tsne_2d[token][1],\n",
    "                \n",
    "                \"tsne_x_3d\" : tsne_3d[token][0],\n",
    "                \"tsne_y_3d\" : tsne_3d[token][1],                \n",
    "                \"tsne_z_3d\" : tsne_3d[token][2],                \n",
    "                \n",
    "                \"umap_x\" : umap_2d[token][0],\n",
    "                \"umap_y\" : umap_2d[token][1],          \n",
    "\n",
    "                \"umap_x_3d\" : umap_3d[token][0],\n",
    "                \"umap_y_3d\" : umap_3d[token][1],                \n",
    "                \"umap_z_3d\" : umap_3d[token][2],                         \n",
    "                \n",
    "                \"pca_x\" : pca_2d[token][0],\n",
    "                \"pca_y\" : pca_2d[token][1],\n",
    "                \n",
    "                \"pca_x_3d\" : pca_3d[token][0],\n",
    "                \"pca_y_3d\" : pca_3d[token][1],                                    \n",
    "                \"pca_z_3d\" : pca_3d[token][2],    \n",
    "                \n",
    "                \"norm\" : calculate_norm(vectors[token])                              \n",
    "            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 284.36it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 325.57it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 369.91it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 89.66it/s]\n"
     ]
    }
   ],
   "source": [
    "for layer in tqdm.tqdm(range(num_layers)): \n",
    "    for head in tqdm.tqdm(range(num_heads)):\n",
    "        avg =  np.average(np.stack([np.stack([token['attention'] for token in attention_dict[layer][head]['tokens'][sentence_starts[i]:sentence_stops[i]]]) for i in range(len(dataset))]),axis=0)\n",
    "        agg_attn_dict[f\"{layer}_{head}\"] = [{\"attention\" : avg[i].tolist()} for i in range(avg.shape[0])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 670.45it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 868.51it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 956.80it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 133.28it/s]\n"
     ]
    }
   ],
   "source": [
    "for layer in tqdm.tqdm(range(num_layers)): \n",
    "    for head in tqdm.tqdm(range(num_heads)):\n",
    "        del point_position_dict[layer][head][\"query\"]\n",
    "        del point_position_dict[layer][head][\"key\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 112.59it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 156.90it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 164.56it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 41.50it/s]\n"
     ]
    }
   ],
   "source": [
    "for layer in tqdm.tqdm(range(num_layers)): \n",
    "    for head in tqdm.tqdm(range(num_heads)):\n",
    "        for i in range(len(point_position_dict[layer][head]['tokens'])):\n",
    "            for data_feature in [\"tsne_x\", \"tsne_y\", \"umap_x\", \"umap_y\", \"norm\", \"tsne_x_3d\", \"tsne_y_3d\", \"tsne_z_3d\", \"umap_x_3d\", \"umap_y_3d\", \"umap_z_3d\", \"pca_x\", \"pca_y\", \"pca_x_3d\", \"pca_y_3d\", \"pca_z_3d\"]:\n",
    "                point_position_dict[layer][head]['tokens'][i][data_feature] = float(point_position_dict[layer][head]['tokens'][i][data_feature])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1184/1184 [00:00<00:00, 4826099.06it/s]\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "tokens = copy.deepcopy(tokens_dict[\"tokens\"])\n",
    "pls = list()\n",
    "for token in tqdm.tqdm(tokens):\n",
    "    token[\"type\"] = \"key\"\n",
    "    tokens_dict[\"tokens\"].append(token)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 175.51it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 193.79it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 195.01it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 55.45it/s]\n"
     ]
    }
   ],
   "source": [
    "for layer in tqdm.tqdm(range(num_layers)): \n",
    "    for head in tqdm.tqdm(range(num_heads)):\n",
    "        for att in attention_dict[layer][head]['tokens']:\n",
    "            att['attention'] = att['attention'].tolist() \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing to files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/home/cameron/repos/attention-viz-bio/web/data/DNABERT/agg_attn.json\", \"w\") as fp:\n",
    "    json.dump(agg_attn_dict , fp) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/home/cameron/repos/attention-viz-bio/web/data/DNABERT/tokens.json\", \"w\") as fp:\n",
    "    json.dump(tokens_dict , fp) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00,  4.37it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00,  3.99it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00,  4.10it/s]\n",
      "100%|██████████| 3/3 [00:02<00:00,  1.38it/s]\n"
     ]
    }
   ],
   "source": [
    "for layer in tqdm.tqdm(range(num_layers)): \n",
    "    for head in tqdm.tqdm(range(num_heads)):\n",
    "        with open(f\"/home/cameron/repos/attention-viz-bio/web/data/DNABERT/attention/layer{layer}_head{head}.json\", \"w\") as fp:\n",
    "            json.dump(attention_dict[layer][head] , fp) \n",
    "        with open(f\"/home/cameron/repos/attention-viz-bio/web/data/DNABERT/byLayerHead/layer{layer}_head{head}.json\", \"w\") as fp:\n",
    "            json.dump(point_position_dict[layer][head] , fp)             \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cam_env_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
