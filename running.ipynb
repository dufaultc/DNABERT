{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cameron/miniconda3/envs/cam_env_2/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n",
      "/home/cameron/miniconda3/envs/cam_env_2/lib/python3.11/site-packages/torch/_utils.py:830: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from src.transformers.models.bert import BertModel\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"zhihan1996/DNA_bert_6\", trust_remote_code=True)\n",
    "model = BertModel.from_pretrained(\"zhihan1996/DNA_bert_6\", trust_remote_code=True).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio import Entrez\n",
    "from Bio import SeqIO\n",
    "Entrez.email = \"dufault77@gmail.com\"\n",
    "def download_sequence(accession, start, end):\n",
    "    handle = Entrez.efetch(db=\"nucleotide\", id=accession, rettype=\"fasta\", strand=1, seq_start=start, seq_stop=end)\n",
    "    seq_record = SeqIO.read(handle, \"fasta\")\n",
    "    handle.close()\n",
    "    return seq_record.seq\n",
    "\n",
    "#sequence_prm1 = download_sequence('NC_000016.10', 11280831, 11281340)\n",
    "\n",
    "#sequence_ = download_sequence('NC_000016.10', 11280831, 11281340)\n",
    "#print(sequence_prm1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "320"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(9779053) - (9778733)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "region_mapping_list = []\n",
    "region_type_mapping_list = []\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "mapping_dict = {}\n",
    "mapping_dict_type = {}\n",
    "mapping_dict[0] = \"cls\"\n",
    "mapping_dict_type[0] = \"cls\"\n",
    "sequence_prm1 = download_sequence('NC_000016.10', 11280831-3, 11281338+3).reverse_complement()\n",
    "for i in range(1, 11):\n",
    "    mapping_dict[i] = \"upstream\"\n",
    "for i in range(11, 205):\n",
    "    mapping_dict[i] = \"exon_1\"\n",
    "for i in range(205, 297):\n",
    "    mapping_dict[i] = \"intron_1\"\n",
    "for i in range(297, 500):\n",
    "    mapping_dict[i] = \"exon_2\"\n",
    "for i in range(500, 508):\n",
    "    mapping_dict[i] = \"downstream\"    \n",
    "mapping_dict[508] = \"sep\"\n",
    "for i in range(1, 11):\n",
    "    mapping_dict_type[i] = \"upstream\"\n",
    "for i in range(11, 205):\n",
    "    mapping_dict_type[i] = \"exon\"\n",
    "for i in range(205, 297):\n",
    "    mapping_dict_type[i] = \"intron\"\n",
    "for i in range(297, 500):\n",
    "    mapping_dict_type[i] = \"exon\"\n",
    "for i in range(500, 508):\n",
    "    mapping_dict_type[i] = \"downstream\"    \n",
    "mapping_dict_type[508] = \"sep\"\n",
    "\n",
    "region_mapping_list.append(mapping_dict)\n",
    "region_type_mapping_list.append(mapping_dict_type)\n",
    "\n",
    "\n",
    "mapping_dict = {}\n",
    "mapping_dict_type = {}\n",
    "mapping_dict[0] = \"cls\"\n",
    "mapping_dict_type[0] = \"cls\"\n",
    "sequence_1 = download_sequence(\"NC_000001.11\",9424658-3, 9425159+3)\n",
    "for i in range(1, 293):\n",
    "    mapping_dict[i] = \"ALU_1\"\n",
    "for i in range(293, 502):\n",
    "    mapping_dict[i] = \"MIR_1\"\n",
    "mapping_dict[502] = \"sep\"\n",
    "for i in range(1, 293):\n",
    "    mapping_dict_type[i] = \"ALU\"\n",
    "for i in range(293, 502):\n",
    "    mapping_dict_type[i] = \"MIR\"\n",
    "mapping_dict_type[502] = \"sep\"\n",
    "region_mapping_list.append(mapping_dict)\n",
    "region_type_mapping_list.append(mapping_dict_type)\n",
    "\n",
    "\n",
    "mapping_dict = {}\n",
    "mapping_dict_type = {}\n",
    "mapping_dict[0] = \"cls\"\n",
    "mapping_dict_type[0] = \"cls\"\n",
    "sequence_2 = download_sequence(\"NC_000001.11\",9530501-3, 9530700+3)\n",
    "for i in range(1, 64):\n",
    "    mapping_dict[i] = \"MIR_1\"\n",
    "for i in range(64, 200):\n",
    "    mapping_dict[i] = \"ALU_1\"\n",
    "mapping_dict[200] = \"sep\"\n",
    "for i in range(1, 64):\n",
    "    mapping_dict_type[i] = \"ALU\"\n",
    "for i in range(64, 200):\n",
    "    mapping_dict_type[i] = \"MIR\"\n",
    "mapping_dict_type[200] = \"sep\"\n",
    "region_mapping_list.append(mapping_dict)\n",
    "region_type_mapping_list.append(mapping_dict_type)\n",
    "\n",
    "\n",
    "mapping_dict = {}\n",
    "mapping_dict_type = {}\n",
    "mapping_dict[0] = \"cls\"\n",
    "mapping_dict_type[0] = \"cls\"\n",
    "sequence_3 = download_sequence(\"NC_000001.11\",9778733-3, 9779053+3)\n",
    "for i in range(1, 321):\n",
    "    mapping_dict[i] = \"ALU_1\"\n",
    "mapping_dict[321] = \"sep\"\n",
    "for i in range(1, 321):\n",
    "    mapping_dict_type[i] = \"ALU\"\n",
    "mapping_dict_type[321] = \"sep\"\n",
    "region_mapping_list.append(mapping_dict)\n",
    "region_type_mapping_list.append(mapping_dict_type)\n",
    "\n",
    "\n",
    "mapping_dict = {}\n",
    "mapping_dict_type = {}\n",
    "mapping_dict[0] = \"cls\"\n",
    "mapping_dict_type[0] = \"cls\"\n",
    "sequence_4 = download_sequence(\"NC_000001.11\",9861780-3, 9861968+3)\n",
    "for i in range(1, 139):\n",
    "    mapping_dict[i] = \"MIR_1\"\n",
    "for i in range(139, 189):\n",
    "    mapping_dict[i] = \"unknown_1\"\n",
    "mapping_dict[189] = \"sep\"\n",
    "for i in range(1, 139):\n",
    "    mapping_dict_type[i] = \"MIR\"\n",
    "for i in range(139, 189):\n",
    "    mapping_dict_type[i] = \"unknown\"    \n",
    "mapping_dict_type[189] = \"sep\"\n",
    "region_mapping_list.append(mapping_dict)\n",
    "region_type_mapping_list.append(mapping_dict_type)\n",
    "\n",
    "\n",
    "mapping_dict = {}\n",
    "mapping_dict_type = {}\n",
    "mapping_dict[0] = \"cls\"\n",
    "mapping_dict_type[0] = \"cls\"\n",
    "sequence_5 = download_sequence(\"NC_000001.11\",9935809-3, 9936144+3)\n",
    "for i in range(1, 41):\n",
    "    mapping_dict[i] = \"unknown_1\"\n",
    "for i in range(41, 336):\n",
    "    mapping_dict[i] = \"ALU_1\"\n",
    "mapping_dict[336] = \"sep\"\n",
    "for i in range(1, 41):\n",
    "    mapping_dict_type[i] = \"unknown\"\n",
    "for i in range(41, 336):\n",
    "    mapping_dict_type[i] = \"ALU\"    \n",
    "mapping_dict_type[336] = \"sep\"\n",
    "region_mapping_list.append(mapping_dict)\n",
    "region_type_mapping_list.append(mapping_dict_type)\n",
    "\n",
    "\n",
    "mapping_dict = {}\n",
    "mapping_dict_type = {}\n",
    "mapping_dict[0] = \"cls\"\n",
    "mapping_dict_type[0] = \"cls\"\n",
    "sequence_6 = download_sequence(\"NC_000001.11\",9955431-3, 9955808+3)\n",
    "for i in range(1, 197):\n",
    "    mapping_dict[i] = \"ALU_1\"\n",
    "for i in range(197, 245):\n",
    "    mapping_dict[i] = \"unknown_1\"\n",
    "for i in range(245, 378):\n",
    "    mapping_dict[i] = \"LINE_1\"    \n",
    "mapping_dict[378] = \"sep\"\n",
    "for i in range(1, 197):\n",
    "    mapping_dict_type[i] = \"ALU\"\n",
    "for i in range(197, 245):\n",
    "    mapping_dict_type[i] = \"unknown\"\n",
    "for i in range(245, 378):\n",
    "    mapping_dict_type[i] = \"LINE\"    \n",
    "mapping_dict_type[378] = \"sep\"\n",
    "region_mapping_list.append(mapping_dict)\n",
    "region_type_mapping_list.append(mapping_dict_type)\n",
    "\n",
    "mapping_dict = {}\n",
    "mapping_dict_type = {}\n",
    "mapping_dict[0] = \"cls\"\n",
    "mapping_dict_type[0] = \"cls\"\n",
    "sequence_7 = download_sequence(\"NC_000001.11\",9640806-3, 9641242+3)\n",
    "for i in range(1,437):\n",
    "    mapping_dict[i] = \"LINE_1\"\n",
    "mapping_dict[437] = \"sep\"\n",
    "for i in range(1, 437):\n",
    "    mapping_dict_type[i] = \"LINE\" \n",
    "mapping_dict_type[437] = \"sep\"\n",
    "region_mapping_list.append(mapping_dict)\n",
    "region_type_mapping_list.append(mapping_dict_type)\n",
    "\n",
    "\n",
    "\n",
    "mapping_dict = {}\n",
    "mapping_dict_type = {}\n",
    "mapping_dict[0] = \"cls\"\n",
    "mapping_dict_type[0] = \"cls\"\n",
    "sequence_8 = download_sequence(\"NC_000001.11\",9406644-3, 9407000+3)\n",
    "for i in range(1, 58):\n",
    "    mapping_dict[i] = \"LINE_1\"\n",
    "for i in range(58, 178):\n",
    "    mapping_dict[i] = \"unknown_1\"\n",
    "for i in range(178, 357):\n",
    "    mapping_dict[i] = \"ALU_1\"    \n",
    "mapping_dict[357] = \"sep\"\n",
    "for i in range(1, 58):\n",
    "    mapping_dict_type[i] = \"LINE\"\n",
    "for i in range(58, 178):\n",
    "    mapping_dict_type[i] = \"unknown\"\n",
    "for i in range(178, 357):\n",
    "    mapping_dict_type[i] = \"ALU\"    \n",
    "mapping_dict_type[357] = \"sep\"\n",
    "region_mapping_list.append(mapping_dict)\n",
    "region_type_mapping_list.append(mapping_dict_type)\n",
    "\n",
    "mapping_dict = {}\n",
    "mapping_dict_type = {}\n",
    "mapping_dict[0] = \"cls\"\n",
    "mapping_dict_type[0] = \"cls\"\n",
    "sequence_fbxw7 = download_sequence(\"NC_000004.12\",152337645-3, 152338109+3)\n",
    "for i in range(1, 11):\n",
    "    mapping_dict[i] = \"upstream\"\n",
    "for i in range(11, 94):\n",
    "    mapping_dict[i] = \"exon_1\"\n",
    "for i in range(94, 166):\n",
    "    mapping_dict[i] = \"intron_1\"\n",
    "for i in range(166, 455):\n",
    "    mapping_dict[i] = \"exon_2\"\n",
    "for i in range(455, 465):\n",
    "    mapping_dict[i] = \"downstream\"        \n",
    "mapping_dict[465] = \"sep\"\n",
    "for i in range(1, 11):\n",
    "    mapping_dict_type[i] = \"upstream\"\n",
    "for i in range(11, 94):\n",
    "    mapping_dict_type[i] = \"exon\"\n",
    "for i in range(94, 166):\n",
    "    mapping_dict_type[i] = \"intron\"\n",
    "for i in range(166, 455):\n",
    "    mapping_dict_type[i] = \"exon\"\n",
    "for i in range(455, 465):\n",
    "    mapping_dict_type[i] = \"downstream\"        \n",
    "mapping_dict_type[465] = \"sep\"\n",
    "region_mapping_list.append(mapping_dict)\n",
    "region_type_mapping_list.append(mapping_dict_type)\n",
    "\n",
    "\n",
    "mapping_dict = {}\n",
    "mapping_dict_type = {}\n",
    "mapping_dict[0] = \"cls\"\n",
    "mapping_dict_type[0] = \"cls\"\n",
    "sequence_krtap = download_sequence(\"NC_000021.9\",30348299-3, 30348626+3).reverse_complement()\n",
    "for i in range(1, 101):\n",
    "    mapping_dict[i] = \"upstream\"\n",
    "for i in range(101, 308):\n",
    "    mapping_dict[i] = \"exon_1\"\n",
    "for i in range(308, 328):\n",
    "    mapping_dict[i] = \"downstream\"        \n",
    "mapping_dict[328] = \"sep\"\n",
    "for i in range(1, 101):\n",
    "    mapping_dict_type[i] = \"upstream\"\n",
    "for i in range(101, 308):\n",
    "    mapping_dict_type[i] = \"exon\"\n",
    "for i in range(308, 328):\n",
    "    mapping_dict_type[i] = \"downstream\"        \n",
    "mapping_dict_type[328] = \"sep\"\n",
    "region_mapping_list.append(mapping_dict)\n",
    "region_type_mapping_list.append(mapping_dict_type)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = [str(sequence_prm1), str(sequence_1), str(sequence_2), str(sequence_3), str(sequence_4), str(sequence_5), str(sequence_6), str(sequence_7), str(sequence_8), str(sequence_fbxw7),str(sequence_krtap)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers = 12\n",
    "num_heads = 12\n",
    "skip = 2\n",
    "start=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_dict = {\"tokens\": list()}\n",
    "vocab_reverse = dict((value, key) for key, value in tokenizer.vocab.items())\n",
    "\n",
    "\n",
    "attention_dict = dict()\n",
    "point_position_dict = dict()\n",
    "point_position_dict = dict()\n",
    "agg_attn_dict = dict()\n",
    "for layer in range(start,num_layers,skip):\n",
    "    attention_dict[layer] = dict()\n",
    "    point_position_dict[layer] = dict()\n",
    "    for head in range(start,num_heads,skip):\n",
    "        agg_attn_dict[f\"{layer}_{head}\"] = list()\n",
    "        attention_dict[layer][head] = dict()\n",
    "        attention_dict[layer][head][\"layer\"] = layer \n",
    "        attention_dict[layer][head][\"head\"] = head\n",
    "        attention_dict[layer][head][\"tokens\"] = list() \n",
    "        point_position_dict[layer][head] = dict()\n",
    "        point_position_dict[layer][head][\"layer\"] = layer \n",
    "        point_position_dict[layer][head][\"head\"] = head\n",
    "        point_position_dict[layer][head][\"tokens\"] = list()\n",
    "        point_position_dict[layer][head][\"query\"] = list()  \n",
    "        point_position_dict[layer][head][\"key\"] = list()         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "sentence_stops = list()\n",
    "sentence_starts = list()\n",
    "pos = 0\n",
    "for seq_num,sequence in enumerate(dataset):\n",
    "    sentence_starts.append(pos)\n",
    "    inputs = tokenizer(\" \".join([sequence[i-3:i+3] for i in range(3, len(sequence)-4, 1)]), return_tensors = 'pt')\n",
    "    out = model(**inputs.to(device), output_attentions=True, return_dict=True)\n",
    "    tokens = [vocab_reverse[x] for x in inputs['input_ids'].tolist()[0]]   \n",
    "    pos = pos+len(tokens)\n",
    "    sentence_stops.append(pos) \n",
    "    for i,value in enumerate(tokens):\n",
    "        single_token = {}\n",
    "        single_token['value'] = value\n",
    "        single_token['region_type'] = region_type_mapping_list[seq_num][i]\n",
    "        single_token['region_name'] = region_mapping_list[seq_num][i]\n",
    "        single_token['type'] = \"query\"\n",
    "        single_token[\"length\"] = len(tokens)\n",
    "        single_token['pos_int'] = i\n",
    "        single_token['position'] = i/(len(tokens)- 1)\n",
    "        single_token['sent_pos'] = (seq_num+1)/(len(dataset))        \n",
    "        single_token['num_sent'] = len(dataset)\n",
    "        single_token['sentence'] = \" \".join(tokens) \n",
    "   \n",
    "        tokens_dict['tokens'].append(single_token)\n",
    "        for layer in range(start,num_layers,skip): \n",
    "            for head in range(start,num_heads,skip):\n",
    "                point_position_dict[layer][head]['query'].append(out.query[layer][head][i].detach().cpu().numpy())\n",
    "                point_position_dict[layer][head]['key'].append(out.key[layer][head][i].detach().cpu().numpy())\n",
    "                attention_dict[layer][head]['tokens'].append({'attention' : out.attentions[layer][0][head][i].detach().cpu().numpy()})\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "329"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:53<00:00,  8.86s/it]\n",
      "100%|██████████| 6/6 [00:50<00:00,  8.36s/it]\n",
      "100%|██████████| 6/6 [00:52<00:00,  8.82s/it]\n",
      "100%|██████████| 6/6 [00:51<00:00,  8.58s/it]\n",
      "100%|██████████| 6/6 [00:52<00:00,  8.82s/it]\n",
      "100%|██████████| 6/6 [00:54<00:00,  9.10s/it]\n",
      "100%|██████████| 6/6 [05:15<00:00, 52.56s/it]\n"
     ]
    }
   ],
   "source": [
    "# Getting point positions\n",
    "\n",
    "#Some code from chatGPT!!!!!!!\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import umap\n",
    "import numpy as np\n",
    "import tqdm\n",
    "\n",
    "def get_pca_embeddings(vectors, n_components=2):\n",
    "    pca = PCA(n_components=n_components)\n",
    "    return pca.fit_transform(vectors)\n",
    "def get_tsne_embeddings(vectors, n_components=2, perplexity=30.0):\n",
    "    tsne = TSNE(n_components=n_components, perplexity=perplexity, n_iter=250)\n",
    "    return tsne.fit_transform(vectors)\n",
    "def get_umap_embeddings(vectors, n_components=2, n_neighbors=15, min_dist=0.1):\n",
    "    umap_model = umap.UMAP(n_components=n_components, n_neighbors=n_neighbors, min_dist=min_dist)\n",
    "    return umap_model.fit_transform(vectors)\n",
    "def calculate_centroid(vectors):\n",
    "    \"\"\"Calculate the centroid of a list of vectors.\"\"\"\n",
    "    return np.mean(vectors, axis=0)\n",
    "def translate_vectors(source_vectors, target_vectors):\n",
    "    \"\"\"Translate source_vectors so their centroid matches that of target_vectors.\"\"\"\n",
    "    source_centroid = calculate_centroid(source_vectors)\n",
    "    target_centroid = calculate_centroid(target_vectors)\n",
    "    translation = target_centroid - source_centroid\n",
    "    translated_vectors = source_vectors + translation\n",
    "    return translated_vectors\n",
    "\n",
    "def calculate_norm(vector):\n",
    "    return np.linalg.norm(vector)\n",
    "\n",
    "for layer in tqdm.tqdm(range(start,num_layers,skip)): \n",
    "    for head in tqdm.tqdm(range(start,num_heads,skip)):\n",
    "        translated_key = translate_vectors(point_position_dict[layer][head]['key'], point_position_dict[layer][head]['query'])\n",
    "        vectors = np.stack(point_position_dict[layer][head]['query'] + [np.array(row) for row in translated_key])\n",
    "        \n",
    "        pca_2d = get_pca_embeddings(vectors, n_components=2)\n",
    "        #pca_3d = get_pca_embeddings(vectors, n_components=3)\n",
    "\n",
    "        tsne_2d = get_tsne_embeddings(vectors, n_components=2)\n",
    "        #tsne_3d = get_tsne_embeddings(vectors, n_components=3)\n",
    "\n",
    "        umap_2d = get_umap_embeddings(vectors, n_components=2)\n",
    "        #umap_3d = get_umap_embeddings(vectors, n_components=3)\n",
    "        \n",
    "        for token in range(umap_2d.shape[0]):\n",
    "            point_position_dict[layer][head]['tokens'].append({\n",
    "                \"tsne_x\" : tsne_2d[token][0],\n",
    "                \"tsne_y\" : tsne_2d[token][1],\n",
    "                \n",
    "                #\"tsne_x_3d\" : tsne_3d[token][0],\n",
    "                #\"tsne_y_3d\" : tsne_3d[token][1],                \n",
    "                #\"tsne_z_3d\" : tsne_3d[token][2],                \n",
    "                \n",
    "                \"umap_x\" : umap_2d[token][0],\n",
    "                \"umap_y\" : umap_2d[token][1],          \n",
    "\n",
    "                #\"umap_x_3d\" : umap_3d[token][0],\n",
    "                #\"umap_y_3d\" : umap_3d[token][1],                \n",
    "                #\"umap_z_3d\" : umap_3d[token][2],                         \n",
    "                \n",
    "                \"pca_x\" : pca_2d[token][0],\n",
    "                \"pca_y\" : pca_2d[token][1],\n",
    "                \n",
    "                #\"pca_x_3d\" : pca_3d[token][0],\n",
    "                #\"pca_y_3d\" : pca_3d[token][1],                                    \n",
    "                #\"pca_z_3d\" : pca_3d[token][2],    \n",
    "                \n",
    "                \"norm\" : calculate_norm(vectors[token])                              \n",
    "            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:00<00:00, 15.59it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 15.97it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 16.02it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 15.10it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 15.72it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 16.07it/s]\n",
      "100%|██████████| 6/6 [00:02<00:00,  2.61it/s]\n"
     ]
    }
   ],
   "source": [
    "for layer in tqdm.tqdm(range(start,num_layers,skip)): \n",
    "    for head in tqdm.tqdm(range(start,num_heads,skip)):\n",
    "        h = [np.stack([token['attention'] for token in attention_dict[layer][head]['tokens'][sentence_starts[i]:sentence_stops[i]]]) for i in range(len(dataset))]\n",
    "        max_h = max([x.shape[0] for x in h])\n",
    "        shaped = [np.pad(x,(0,max_h - x.shape[0])) for x in h]\n",
    "        avg =  np.average(np.stack(shaped),axis=0)\n",
    "        agg_attn_dict[f\"{layer}_{head}\"] = [{\"attention\" : [round(x,5) for x in avg[i].tolist()]} for i in range(avg.shape[0])]\n",
    "        \n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:00<00:00, 109.25it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 141.32it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 134.05it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 139.00it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 144.89it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 138.19it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 20.46it/s]\n"
     ]
    }
   ],
   "source": [
    "for layer in tqdm.tqdm(range(start,num_layers,skip)): \n",
    "    for head in tqdm.tqdm(range(start,num_heads,skip)):\n",
    "        del point_position_dict[layer][head][\"query\"]\n",
    "        del point_position_dict[layer][head][\"key\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:00<00:00, 42.51it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 43.53it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 44.95it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 41.32it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 45.26it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 44.22it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00,  7.15it/s]\n"
     ]
    }
   ],
   "source": [
    "for layer in tqdm.tqdm(range(start,num_layers,skip)): \n",
    "    for head in tqdm.tqdm(range(start,num_heads,skip)):\n",
    "        for i in range(len(point_position_dict[layer][head]['tokens'])):\n",
    "            for data_feature in [\"tsne_x\", \"tsne_y\", \"umap_x\", \"umap_y\", \"norm\", \"pca_x\", \"pca_y\"]:\n",
    "                point_position_dict[layer][head]['tokens'][i][data_feature] = round(float(point_position_dict[layer][head]['tokens'][i][data_feature]),5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4032/4032 [00:00<00:00, 7593818.47it/s]\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "tokens = copy.deepcopy(tokens_dict[\"tokens\"])\n",
    "pls = list()\n",
    "for token in tqdm.tqdm(tokens):\n",
    "    token[\"type\"] = \"key\"\n",
    "    tokens_dict[\"tokens\"].append(token)\n",
    "del tokens\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing to files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/home/cameron/repos/attention-viz-bio/web/data/DNABERT/agg_attn.json\", \"w\") as fp:\n",
    "    json.dump(agg_attn_dict , fp) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "del agg_attn_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/home/cameron/repos/attention-viz-bio/web/data/DNABERT/tokens.json\", \"w\") as fp:\n",
    "    json.dump(tokens_dict , fp) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "del tokens_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:01<00:00,  3.04it/s]\n",
      "100%|██████████| 6/6 [00:02<00:00,  2.93it/s]\n",
      "100%|██████████| 6/6 [00:02<00:00,  2.91it/s]\n",
      "100%|██████████| 6/6 [00:02<00:00,  2.67it/s]\n",
      "100%|██████████| 6/6 [00:03<00:00,  1.93it/s]\n",
      "100%|██████████| 6/6 [00:02<00:00,  2.19it/s]\n",
      "100%|██████████| 6/6 [00:14<00:00,  2.37s/it]\n"
     ]
    }
   ],
   "source": [
    "for layer in tqdm.tqdm(range(start,num_layers,skip)): \n",
    "    for head in tqdm.tqdm(range(start,num_heads,skip)):\n",
    "        for att in attention_dict[layer][head]['tokens']:\n",
    "            att['attention'] = [round(x,5) for x in att['attention'].tolist()] \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:03<00:00,  1.72it/s]\n",
      "100%|██████████| 6/6 [00:03<00:00,  1.66it/s]\n",
      "100%|██████████| 6/6 [00:03<00:00,  1.51it/s]\n",
      "100%|██████████| 6/6 [00:02<00:00,  2.08it/s]\n",
      "100%|██████████| 6/6 [00:03<00:00,  1.81it/s]\n",
      "100%|██████████| 6/6 [00:03<00:00,  1.71it/s]\n",
      "100%|██████████| 6/6 [00:20<00:00,  3.46s/it]\n"
     ]
    }
   ],
   "source": [
    "for layer in tqdm.tqdm(range(start,num_layers,skip)): \n",
    "    for head in tqdm.tqdm(range(start,num_heads,skip)):\n",
    "        with open(f\"/home/cameron/repos/attention-viz-bio/web/data/DNABERT/attention/layer{layer}_head{head}.json\", \"w\") as fp:\n",
    "            json.dump(attention_dict[layer][head] , fp) \n",
    "        with open(f\"/home/cameron/repos/attention-viz-bio/web/data/DNABERT/byLayerHead/layer{layer}_head{head}.json\", \"w\") as fp:\n",
    "            json.dump(point_position_dict[layer][head] , fp)             \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing aggregation things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_dict_aggregate = {\"tokens\": list()}\n",
    "vocab_reverse = dict((value, key) for key, value in tokenizer.vocab.items())\n",
    "\n",
    "\n",
    "attention_dict_aggregate = dict()\n",
    "point_position_dict_aggregate = dict()\n",
    "agg_attn_dict_aggregate = dict()\n",
    "for layer in range(start,num_layers,skip):\n",
    "    attention_dict_aggregate[layer] = dict()\n",
    "    point_position_dict_aggregate[layer] = dict()\n",
    "    for head in range(start,num_heads,skip):\n",
    "        agg_attn_dict_aggregate[f\"{layer}_{head}\"] = list()\n",
    "        attention_dict_aggregate[layer][head] = dict()\n",
    "        attention_dict_aggregate[layer][head][\"layer\"] = layer \n",
    "        attention_dict_aggregate[layer][head][\"head\"] = head\n",
    "        attention_dict_aggregate[layer][head][\"tokens\"] = list() \n",
    "        point_position_dict_aggregate[layer][head] = dict()\n",
    "        point_position_dict_aggregate[layer][head][\"layer\"] = layer \n",
    "        point_position_dict_aggregate[layer][head][\"head\"] = head\n",
    "        point_position_dict_aggregate[layer][head][\"tokens\"] = list()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_list = list(region_mapping_list[0].values())\n",
    "unique_list = []\n",
    "for item in original_list:\n",
    "    if item not in unique_list:\n",
    "        unique_list.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_dicts(dict_list):\n",
    "    if not dict_list:\n",
    "        return {}\n",
    "    \n",
    "    # Initialize a dictionary to store the sum of values for each key\n",
    "    sum_dict = {key: 0 for key in dict_list[0].keys()}\n",
    "\n",
    "    # Sum up the values for each key\n",
    "    for d in dict_list:\n",
    "        for key in sum_dict.keys():\n",
    "            sum_dict[key] += d[key]\n",
    "\n",
    "    # Calculate the average for each key\n",
    "    avg_dict = {key: value / len(dict_list) for key, value in sum_dict.items()}\n",
    "    #avg_dict['norm'] = sum_dict['norm']\n",
    "\n",
    "    return avg_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "sentence_stops_aggregate = list()\n",
    "sentence_starts_aggregate = list()\n",
    "pos = 0\n",
    "for seq_num,sequence in enumerate(dataset):\n",
    "    sentence_starts_aggregate.append(pos)    \n",
    "    original_list = list(region_mapping_list[seq_num].values())\n",
    "    tokens = []\n",
    "    change_list = []\n",
    "    for i,item in enumerate(original_list):\n",
    "        if item not in tokens:\n",
    "            change_list.append(i)\n",
    "            tokens.append(item)\n",
    "    change_list.append(len(original_list))\n",
    "    pos = pos+len(tokens)\n",
    "    sentence_stops_aggregate.append(pos) \n",
    "    for i,value in enumerate(tokens):\n",
    "        single_token = {}\n",
    "        single_token['value'] = value\n",
    "        single_token['region_type'] = value.split(\"_\")[0]\n",
    "        single_token['region_name'] = value\n",
    "        single_token['type'] = \"query\"\n",
    "        single_token[\"length\"] = len(tokens)\n",
    "        single_token['pos_int'] = i\n",
    "        single_token['position'] = i/(len(tokens)- 1)\n",
    "        single_token['sent_pos'] = (seq_num+1)/(len(dataset))\n",
    "        single_token['num_sent'] = len(dataset)\n",
    "        single_token['sentence'] = \" \".join(tokens) \n",
    "   \n",
    "        tokens_dict_aggregate['tokens'].append(single_token)\n",
    "    for layer in range(start,num_layers,skip): \n",
    "        for head in range(start,num_heads,skip):       \n",
    "            x =  [[attention_dict[layer][head]['tokens'][j+sentence_starts[seq_num]]['attention'][change_list[i]:change_list[i+1]] for i in range(len(tokens))] for j in range(len(original_list))  ]               \n",
    "            out = [[np.average(attention_dict[layer][head]['tokens'][j+sentence_starts[seq_num]]['attention'][change_list[i]:change_list[i+1]]) for i in range(len(tokens))] for j in range(len(original_list))  ]\n",
    "            for i in range(len(tokens)):\n",
    "                point_position_dict_aggregate[layer][head][\"tokens\"].append(average_dicts(point_position_dict[layer][head]['tokens'][(sentence_starts[seq_num]+change_list[i]):(sentence_starts[seq_num]+change_list[i+1])]))\n",
    "                attention_dict_aggregate[layer][head]['tokens'].append({'attention' : np.average(np.array(out[change_list[i]:change_list[i+1]]),axis=0)})\n",
    "                \n",
    "\n",
    "                \n",
    "pos = 0\n",
    "for seq_num,sequence in enumerate(dataset):\n",
    "    original_list = list(region_mapping_list[seq_num].values())\n",
    "    tokens = []\n",
    "    change_list = []\n",
    "    for i,item in enumerate(original_list):\n",
    "        if item not in tokens:\n",
    "            change_list.append(i)\n",
    "            tokens.append(item)\n",
    "    change_list.append(len(original_list))\n",
    "    for layer in range(start,num_layers,skip): \n",
    "        for head in range(start,num_heads,skip):         \n",
    "            for i in range(len(tokens)):\n",
    "                point_position_dict_aggregate[layer][head][\"tokens\"].append(average_dicts(point_position_dict[layer][head]['tokens'][(sentence_stops[-1]+sentence_starts[seq_num]+change_list[i]):(sentence_stops[-1]+sentence_starts[seq_num]+change_list[i+1])]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:00<00:00, 1594.69it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 2826.99it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 1497.70it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 3558.52it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 2798.38it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 1662.32it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 211.47it/s]\n"
     ]
    }
   ],
   "source": [
    "for layer in tqdm.tqdm(range(start,num_layers,skip)): \n",
    "    for head in tqdm.tqdm(range(start,num_heads,skip)):\n",
    "        h = [np.stack([token['attention'] for token in attention_dict_aggregate[layer][head]['tokens'][sentence_starts_aggregate[i]:sentence_stops_aggregate[i]]]) for i in range(len(dataset))]\n",
    "        max_h = max([x.shape[0] for x in h])\n",
    "        shaped = [np.pad(x,(0,max_h - x.shape[0])) for x in h]\n",
    "        avg =  np.average(np.stack(shaped),axis=0)\n",
    "        agg_attn_dict_aggregate[f\"{layer}_{head}\"] = [{\"attention\" : [round(x,5) for x in avg[i].tolist()]} for i in range(avg.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:00<00:00, 5783.92it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 3663.15it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 5055.41it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 7458.75it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 8551.08it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 7667.83it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 319.52it/s]\n"
     ]
    }
   ],
   "source": [
    "for layer in tqdm.tqdm(range(start,num_layers,skip)): \n",
    "    for head in tqdm.tqdm(range(start,num_heads,skip)):\n",
    "        for i in range(len(point_position_dict_aggregate[layer][head]['tokens'])):\n",
    "            for data_feature in [\"tsne_x\", \"tsne_y\", \"umap_x\", \"umap_y\", \"norm\", \"pca_x\", \"pca_y\"]:\n",
    "                point_position_dict_aggregate[layer][head]['tokens'][i][data_feature] = float(point_position_dict_aggregate[layer][head]['tokens'][i][data_feature])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [00:00<00:00, 732566.79it/s]\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "tokens = copy.deepcopy(tokens_dict_aggregate[\"tokens\"])\n",
    "pls = list()\n",
    "for token in tqdm.tqdm(tokens):\n",
    "    token[\"type\"] = \"key\"\n",
    "    tokens_dict_aggregate[\"tokens\"].append(token)\n",
    "del tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:00<00:00, 15270.52it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 21112.27it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 18262.57it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 22878.02it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 17835.45it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 23003.50it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 393.23it/s]\n"
     ]
    }
   ],
   "source": [
    "for layer in tqdm.tqdm(range(start,num_layers,skip)): \n",
    "    for head in tqdm.tqdm(range(start,num_heads,skip)):\n",
    "        for att in attention_dict_aggregate[layer][head]['tokens']:\n",
    "            att['attention'] = att['attention'].tolist() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing Aggregate to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"/home/cameron/repos/attention-viz-bio/web/data/DNABERT_agg/agg_attn.json\", \"w\") as fp:\n",
    "    json.dump(agg_attn_dict_aggregate , fp) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/home/cameron/repos/attention-viz-bio/web/data/DNABERT_agg/tokens.json\", \"w\") as fp:\n",
    "    json.dump(tokens_dict_aggregate , fp) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:00<00:00, 828.34it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 407.62it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 530.71it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 685.85it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 675.01it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 838.94it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 84.25it/s]\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "## Writing to files\n",
    "import json\n",
    "\n",
    "for layer in tqdm.tqdm(range(start,num_layers,skip)): \n",
    "    for head in tqdm.tqdm(range(start,num_heads,skip)):\n",
    "        with open(f\"/home/cameron/repos/attention-viz-bio/web/data/DNABERT_agg/attention/layer{layer}_head{head}.json\", \"w\") as fp:\n",
    "            json.dump(attention_dict_aggregate[layer][head] , fp) \n",
    "        with open(f\"/home/cameron/repos/attention-viz-bio/web/data/DNABERT_agg/byLayerHead/layer{layer}_head{head}.json\", \"w\") as fp:\n",
    "            json.dump(point_position_dict_aggregate[layer][head] , fp)             \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cam_env_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
