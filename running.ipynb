{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cameron/miniconda3/envs/cam_env_2/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n",
      "/home/cameron/miniconda3/envs/cam_env_2/lib/python3.11/site-packages/torch/_utils.py:830: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from src.transformers.models.bert import BertModel\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"zhihan1996/DNA_bert_6\", trust_remote_code=True)\n",
    "model = BertModel.from_pretrained(\"zhihan1996/DNA_bert_6\", trust_remote_code=True).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_dict = {\"tokens\": list()}\n",
    "vocab_reverse = dict((value, key) for key, value in tokenizer.vocab.items())\n",
    "\n",
    "attention_dict = dict()\n",
    "point_position_dict = dict()\n",
    "agg_attn_dict = dict()\n",
    "for layer in range(12):\n",
    "    attention_dict[layer] = dict()\n",
    "    point_position_dict[layer] = dict()\n",
    "    for head in range(12):\n",
    "        agg_attn_dict[f\"{layer}_{head}\"] = list()\n",
    "        attention_dict[layer][head] = dict()\n",
    "        attention_dict[layer][head][\"layer\"] = layer \n",
    "        attention_dict[layer][head][\"head\"] = head\n",
    "        attention_dict[layer][head][\"tokens\"] = list() \n",
    "        point_position_dict[layer][head] = dict()\n",
    "        point_position_dict[layer][head][\"layer\"] = layer \n",
    "        point_position_dict[layer][head][\"head\"] = head\n",
    "        point_position_dict[layer][head][\"tokens\"] = list()\n",
    "        point_position_dict[layer][head][\"query\"] = list()  \n",
    "        point_position_dict[layer][head][\"key\"] = list()         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "dna = \"ACGTAGCATCGGATCTATCTATCGACACTTGGTTATCGATCTACGAGCATCTCGTTAGC\"\n",
    "dataset = [dna]\n",
    "for sequence in dataset:\n",
    "    inputs = tokenizer(\" \".join([sequence[i:i+6] for i in range(0, len(sequence), 1)]), return_tensors = 'pt')\n",
    "    out = model(**inputs.to(device), output_attentions=True, return_dict=True)\n",
    "    tokens = [vocab_reverse[x] for x in inputs['input_ids'].tolist()[0]]    \n",
    "    for i,value in enumerate(tokens):\n",
    "        single_token = {}\n",
    "        single_token['value'] = value\n",
    "        single_token['type'] = \"query\"\n",
    "        single_token[\"length\"] = len(tokens)\n",
    "        single_token['pos_int'] = i\n",
    "        single_token['position'] = i/(len(tokens)- 1)\n",
    "        single_token['sentence'] = \" \".join(tokens) \n",
    "   \n",
    "        tokens_dict['tokens'].append(single_token)\n",
    "        for layer in range(12): \n",
    "            for head in range(12):\n",
    "                point_position_dict[layer][head]['query'].append(out.query[layer][head][i].detach().cpu().numpy())\n",
    "                point_position_dict[layer][head]['key'].append(out.key[layer][head][i].detach().cpu().numpy())\n",
    "                attention_dict[layer][head]['tokens'].append({'attention' : out.attentions[layer][0][head][i].detach().cpu().numpy()})\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/12 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:42<00:00,  3.57s/it]\n",
      "100%|██████████| 12/12 [00:43<00:00,  3.59s/it]\n",
      "100%|██████████| 12/12 [00:45<00:00,  3.80s/it]\n",
      "100%|██████████| 12/12 [00:44<00:00,  3.70s/it]\n",
      "100%|██████████| 12/12 [00:44<00:00,  3.68s/it]\n",
      "100%|██████████| 12/12 [00:44<00:00,  3.70s/it]\n",
      "100%|██████████| 12/12 [00:45<00:00,  3.80s/it]\n",
      "100%|██████████| 12/12 [00:44<00:00,  3.75s/it]\n",
      "100%|██████████| 12/12 [00:49<00:00,  4.10s/it]\n",
      "100%|██████████| 12/12 [00:46<00:00,  3.90s/it]\n",
      "100%|██████████| 12/12 [00:46<00:00,  3.88s/it]\n",
      "100%|██████████| 12/12 [00:46<00:00,  3.83s/it]\n",
      "100%|██████████| 12/12 [09:03<00:00, 45.30s/it]\n"
     ]
    }
   ],
   "source": [
    "# Getting point positions\n",
    "\n",
    "#Some code from chatGPT!!!!!!!\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import umap\n",
    "import numpy as np\n",
    "import tqdm\n",
    "\n",
    "def get_pca_embeddings(vectors, n_components=2):\n",
    "    pca = PCA(n_components=n_components)\n",
    "    return pca.fit_transform(vectors)\n",
    "def get_tsne_embeddings(vectors, n_components=2, perplexity=30.0):\n",
    "    tsne = TSNE(n_components=n_components, perplexity=perplexity)\n",
    "    return tsne.fit_transform(vectors)\n",
    "def get_umap_embeddings(vectors, n_components=2, n_neighbors=15, min_dist=0.1):\n",
    "    umap_model = umap.UMAP(n_components=n_components, n_neighbors=n_neighbors, min_dist=min_dist)\n",
    "    return umap_model.fit_transform(vectors)\n",
    "\n",
    "\n",
    "def calculate_norm(vector):\n",
    "    return np.linalg.norm(vector)\n",
    "\n",
    "for layer in tqdm.tqdm(range(12)): \n",
    "    for head in tqdm.tqdm(range(12)):\n",
    "        for vector in ['query', 'key']:\n",
    "            vectors = np.stack(point_position_dict[layer][head][vector])\n",
    "\n",
    "            pca_2d = get_pca_embeddings(vectors, n_components=2)\n",
    "            pca_3d = get_pca_embeddings(vectors, n_components=3)\n",
    "\n",
    "            tsne_2d = get_tsne_embeddings(vectors, n_components=2)\n",
    "            tsne_3d = get_tsne_embeddings(vectors, n_components=3)\n",
    "\n",
    "            umap_2d = get_umap_embeddings(vectors, n_components=2)\n",
    "            umap_3d = get_umap_embeddings(vectors, n_components=3)\n",
    "            \n",
    "            for token in range(umap_2d.shape[0]):\n",
    "                point_position_dict[layer][head]['tokens'].append({\n",
    "                    \"tsne_x\" : tsne_2d[token][0],\n",
    "                    \"tsne_y\" : tsne_2d[token][1],\n",
    "                    \n",
    "                    \"tsne_x_3d\" : tsne_3d[token][0],\n",
    "                    \"tsne_y_3d\" : tsne_3d[token][1],                \n",
    "                    \"tsne_z_3d\" : tsne_3d[token][2],                \n",
    "                    \n",
    "                    \"umap_x\" : umap_2d[token][0],\n",
    "                    \"umap_y\" : umap_2d[token][1],          \n",
    "\n",
    "                    \"umap_x_3d\" : umap_3d[token][0],\n",
    "                    \"umap_y_3d\" : umap_3d[token][1],                \n",
    "                    \"umap_z_3d\" : umap_3d[token][2],                         \n",
    "                    \n",
    "                    \"pca_x\" : pca_2d[token][0],\n",
    "                    \"pca_y\" : pca_2d[token][1],\n",
    "                    \n",
    "                    \"pca_x_3d\" : pca_3d[token][0],\n",
    "                    \"pca_y_3d\" : pca_3d[token][1],                                    \n",
    "                    \"pca_z_3d\" : pca_3d[token][2],    \n",
    "                    \n",
    "                    \"norm\" : calculate_norm(vectors[token])                              \n",
    "                })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(61, 2)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for layer in tqdm.tqdm(range(12)): \n",
    "    for head in tqdm.tqdm(range(12)):\n",
    "        attention_dict[]np.average(np.stack([token['attention'] for token in attention_dict[0][0]['tokens']]), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(61, 61)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.stack([token['attention'] for token in attention_dict[0][0]['tokens']]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.01639344, 0.01639344, 0.01639344, 0.01639345, 0.01639344,\n",
       "       0.01639344, 0.01639345, 0.01639344, 0.01639344, 0.01639344,\n",
       "       0.01639344, 0.01639344, 0.01639344, 0.01639345, 0.01639344,\n",
       "       0.01639344, 0.01639344, 0.01639344, 0.01639344, 0.01639345,\n",
       "       0.01639344, 0.01639344, 0.01639344, 0.01639345, 0.01639344,\n",
       "       0.01639344, 0.01639344, 0.01639344, 0.01639345, 0.01639344,\n",
       "       0.01639344, 0.01639344, 0.01639344, 0.01639344, 0.01639344,\n",
       "       0.01639344, 0.01639344, 0.01639344, 0.01639344, 0.01639344,\n",
       "       0.01639345, 0.01639344, 0.01639344, 0.01639344, 0.01639344,\n",
       "       0.01639345, 0.01639345, 0.01639345, 0.01639344, 0.01639344,\n",
       "       0.01639344, 0.01639345, 0.01639344, 0.01639344, 0.01639344,\n",
       "       0.01639345, 0.01639344, 0.01639345, 0.01639344, 0.01639344,\n",
       "       0.01639344], dtype=float32)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.average(np.stack([token['attention'] for token in attention_dict[0][0]['tokens']]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "inputs = tokenizer(\" \".join([dna[i:i+6] for i in range(0, len(dna)-6, 1)]), return_tensors = 'pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(out.attentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 12, 61, 61])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.attentions[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0209, 0.0144, 0.0059, 0.0198, 0.0140, 0.0352, 0.0210, 0.0125, 0.0037,\n",
       "        0.0059, 0.0109, 0.0372, 0.0382, 0.0058, 0.0201, 0.0153, 0.0225, 0.0069,\n",
       "        0.0243, 0.0233, 0.0055, 0.0011, 0.0012, 0.0033, 0.0259, 0.0102, 0.0288,\n",
       "        0.0083, 0.0144, 0.0206, 0.0613, 0.0212, 0.0308, 0.0196, 0.0060, 0.0021,\n",
       "        0.0089, 0.0165, 0.0643, 0.0132, 0.0056, 0.0143, 0.0109, 0.0035, 0.0046,\n",
       "        0.0101, 0.0302, 0.0428, 0.0203, 0.0044, 0.0099, 0.0223, 0.0420, 0.0128,\n",
       "        0.0215, 0.0047, 0.0051, 0.0047, 0.0036, 0.0041, 0.0014],\n",
       "       device='cuda:0', grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.attentions[0][0][1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] ACGTAG CGTAGC GTAGCA TAGCAT AGCATC GCATCG CATCGG ATCGGA TCGGAT CGGATC GGATCT GATCTA ATCTAT TCTATC CTATCT TATCTA ATCTAT TCTATC CTATCG TATCGA ATCGAC TCGACA CGACAC GACACT ACACTT CACTTG ACTTGG CTTGGT TTGGTT TGGTTA GGTTAT GTTATC TTATCG TATCGA ATCGAT TCGATC CGATCT GATCTA ATCTAC TCTACG CTACGA TACGAG ACGAGC CGAGCA GAGCAT AGCATC GCATCT CATCTC ATCTCG TCTCGT CTCGTT TCGTTA CGTTAG [SEP]'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join([vocab_reverse[x] for x in inputs['input_ids'].tolist()[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model(**inputs.to(device), output_attentions=True, return_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12, 12, 61, 64])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.query.shape[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0622, -0.0385,  0.1013,  ..., -0.0403, -0.0234, -0.0343],\n",
       "         [ 0.1521,  0.0156,  0.0175,  ..., -0.1897,  0.0935,  0.0241],\n",
       "         [ 0.0246,  0.0086,  0.0833,  ..., -0.0325,  0.0045,  0.0782],\n",
       "         ...,\n",
       "         [-0.0537,  0.1426,  0.0506,  ...,  0.0769,  0.0726,  0.1250],\n",
       "         [ 0.1343, -0.1633, -0.0138,  ..., -0.4566, -0.1181, -0.0063],\n",
       "         [-0.0328, -0.0395,  0.0512,  ..., -0.0384, -0.0244, -0.0343]]],\n",
       "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.embeddings.forward(inputs.to(device)[\"input_ids\"], inputs.to(device)[\"token_type_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_states = model.encoder.forward(model.embeddings.forward(inputs.to(device)[\"input_ids\"], inputs.to(device)[\"token_type_ids\"]),inputs.to(device)[\"attention_mask\"] ,output_all_encoded_layers =True) # [1, sequence_length, 768]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([17, 768])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_states[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_states[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([768])\n",
      "torch.Size([768])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# embedding with mean pooling\n",
    "embedding_mean = torch.mean(hidden_states[0], dim=0)\n",
    "print(embedding_mean.shape) # expect to be 768\n",
    "\n",
    "# embedding with max pooling\n",
    "embedding_max = torch.max(hidden_states[0], dim=0)[0]\n",
    "print(embedding_max.shape) # expect to be 768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.current_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 17, 768])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_states.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4096"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(4101, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cam_env_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
